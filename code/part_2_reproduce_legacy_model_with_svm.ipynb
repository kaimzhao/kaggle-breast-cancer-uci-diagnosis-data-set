{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Project - Breast Cancer Wisconsin (Diagnostic) Data Set\n",
    "# Part 2 - Reproduce 1994 Legacy Model with Support Vector Machine\n",
    "\n",
    "## Summary:\n",
    "This section reproduces the classificaiton model described in the 1994 research paper <em>[Breast Cancer Diagnosis and Prognosis via Linear Programming](https://www.semanticscholar.org/paper/Breast-Cancer-Diagnosis-and-Prognosis-Via-Linear-Mangasarian-Street/3721bb14b16e866115c906336e9d70db096c05b9)</em> published/revised by Olvi L. Mangasarian, W. Nick Street & William H. Wolberg.\n",
    "\n",
    "As shown in **Part 1 - EDA**, total 30 features were computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. Based on the mathematical formula and loss function described in the paper, the diagnostic model, <em>multisurface method Tree (MSM-T)</em>, is analogous to **[Support Vector Machine](https://en.wikipedia.org/wiki/Support-vector_machine)**. The best results were obtained with one plane the the following three features:\n",
    "\n",
    "1. extreme area, \n",
    "2. extreme smoothness, and\n",
    "3. mean texture.\n",
    "\n",
    "The accuracy of the model estimated with cross-validation was 97.5%. The paper then used <em>Parzen window kernel</em> technique (the mechanism is analogus to DBSCAN) to construct the probability density function for benign and malignant along the separating plane.\n",
    "\n",
    "This part will reproduce the model for the diagnostic portion of the paper and reconstruct the probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve, recall_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from data.csv\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "# Data and features\n",
    "print(f'df dimension: {df.shape}')\n",
    "# Check first 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Categorize target variable \"diagnosis\" for visual exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variable for diagnosis\n",
    "df = pd.get_dummies(df, columns=['diagnosis'], drop_first=True)\n",
    "\n",
    "# Check data inbalance\n",
    "df['diagnosis_M'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Scatter plot for three modeled features\n",
    "fig = px.scatter_3d(df, \n",
    "                    x='texture_mean', \n",
    "                    y='area_worst', \n",
    "                    z='smoothness_worst', \n",
    "                    color='diagnosis_M',\n",
    "                    size='area_worst'\n",
    "                   )\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap color scheme\n",
    "heatmap_scheme = sns.diverging_palette(220, 10, sep=80, n=100)\n",
    "\n",
    "# Heatmap for selected features\n",
    "plt.figure(figsize=(6, 5))\n",
    "# Figure - Show correlation between same features in different groups (e.g., mean, se, and worst)\n",
    "sns.heatmap(df[['texture_mean','area_worst','smoothness_worst']].corr(), \n",
    "            annot=True, annot_kws=None, fmt=\".2f\",                                   # Annotate format\n",
    "            cmap=heatmap_scheme, vmin=-1, vmax=1,                                    # Color bar scale\n",
    "            linewidths=0.1, linestyle='-.' )                                         # Gridline format\n",
    "plt.title('Correlation - Selected Features', fontsize=13, pad=10)\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_legacy = ['texture_mean', 'area_worst', 'smoothness_worst']\n",
    "X = df[feat_legacy]\n",
    "y = df['diagnosis_M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Pipeline & GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pipeline\n",
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('svc', SVC(random_state=42, probability=True))\n",
    "])\n",
    "\n",
    "# Model parameters for GridSearch\n",
    "param_grid = {\n",
    "    'svc__kernel': ['rbf'],\n",
    "    'svc__C': np.logspace(-3, 3, 7),\n",
    "    'svc__gamma': np.logspace(-3, 3, 7)\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "search = GridSearchCV(pipe, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "search.fit(X, y)\n",
    "print(f'The best parameters are: {search.best_params_}.')\n",
    "\n",
    "# Predit Train\n",
    "y_pred = search.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The prediction accuracy, estimated with cross-validation is {round(search.best_score_*100, 1)}%.')\n",
    "print(f'The training recall is {round(recall_score(y, y_pred)*100, 1)}%.')\n",
    "print(f'The training F-score is {round(f1_score(y, y_pred)*100, 1)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of \n",
    "m_prob = [i[1] for i in search.predict_proba(X)]\n",
    "\n",
    "pred_df = pd.DataFrame({'true_y': y, 'pred_y': m_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_y'] == 0]['pred_y'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Diagnosis = Benign')\n",
    "plt.hist(pred_df[pred_df['true_y'] == 1]['pred_y'],\n",
    "         bins=25,\n",
    "         color='red',\n",
    "         alpha = 0.6,\n",
    "         label='Diagnosis = Malignant')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Probability Distribution of Prediction = Maglignant', fontsize=18, pad=20)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Predicted Probability of Malignant', fontsize=14)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model output probability the top 5 misclassfied maglignant cells\n",
    "pred_df[pred_df['true_y'] == 1].sort_values(by='pred_y').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matris\n",
    "pd.DataFrame(confusion_matrix(y, y_pred), columns=['pred_N', 'pred_T'], index=['actual_N', 'actual_T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction accuracy of the model based on cross-validation is 97.5%, which matches that in the 1994 paper. As shown in the probability distribution figure above, for certain maglignant cells, the probability being correctly classified is low (nearly 0). This implies the similarity of the chose features between certain cancerous cell and healthy cell and there is not enough info for the model to distinguish between the two. To minimize the false negative diagnosis, we have to set the probability threshold to 0, or simply assume all cells are malignant (which is not a very useful model). The proposed solution to improve the model is to add more features, which will be discussed in **Part 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC Score & ROC AUC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values.\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_y', 'pred_y', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_y', 'pred_y', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Receiver Operating Characteristic Curve', fontsize=18, pad=20)\n",
    "plt.ylabel('Sensitivity', fontsize=14)\n",
    "plt.xlabel('1 - Specificity', fontsize=14)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the ROC AUC curve shows the model is accurage. For medical diagnosis, the priority is to minimize the false negative. Seeking for further improvement is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of C & Gamma - Heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the \n",
    "C = np.logspace(-3, 3, 7)\n",
    "gamma = np.logspace(-3, 3, 7)\n",
    "val_scores = search.cv_results_['mean_test_score'].reshape(len(C),len(gamma))\n",
    "\n",
    "### The following code in this cell is from Sklearn documentary\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#rbf-svm-parameters\n",
    "\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "# Draw heatmap of the validation accuracy as a function of gamma and C\n",
    "#\n",
    "# The score are encoded as colors with the hot colormap which varies from dark\n",
    "# red to bright yellow. As the most interesting scores are all located in the\n",
    "# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n",
    "# as to make it easier to visualize the small variations of score values in the\n",
    "# interesting range while not brutally collapsing all the low score values to\n",
    "# the same color.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=0, right=1.0, bottom=0, top=1.0)\n",
    "plt.imshow(val_scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma)), gamma, rotation=45)\n",
    "plt.yticks(np.arange(len(C)), C)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the heatmap abbove, the model performs best along the diagonal when the value of `gamma` is low and that of `C` is high. \n",
    "\n",
    "<span style=\"color:blue\">*Intuitively, the `gamma` parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The `gamma` parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "The `C` parameter trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of `C`, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower `C` will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words`C` behaves as a regularization parameter in the SVM.*</span>\n",
    "\n",
    "Above text directly cited from Sklearn documentary. As shown above, once `gamma` becomes too large, the radius of the influece of the support vector becomes so small that regularization can no longer help prevent overfitting. On the other hand, too much regularization (small `C`) makes the model insensitive to the variation of `gamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prediction to the original dataframe\n",
    "df['diagnosis_M_Pred'] = y_pred\n",
    "\n",
    "# Relabel class for 3D-Plot \n",
    "# benigh = 3, false negative = 2, falst positive = 1, malignant = 0\n",
    "y_compare=[]\n",
    "for index, diag in enumerate(df['diagnosis_M']):\n",
    "    if (diag != df['diagnosis_M_Pred'][index]) & (diag == 0):\n",
    "        y_compare.append(int(2))\n",
    "    elif (diag != df['diagnosis_M_Pred'][index]) & (diag == 1):\n",
    "        y_compare.append(int(1))\n",
    "    elif diag == 0:\n",
    "        y_compare.append(int(3))\n",
    "    else:\n",
    "        y_compare.append(int(0))\n",
    "df['diag_compare'] = y_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=df['texture_mean'], \n",
    "    y=df['area_worst'], \n",
    "    z=df['smoothness_worst'] ,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=df['diag_compare'], # set color to an array/list of desired values\n",
    "        colorscale= 'rainbow',   # choose a colorscale\n",
    "        colorbar=None,\n",
    "        opacity=0.6\n",
    "    )\n",
    ")])\n",
    "\n",
    "# tight layout\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D-plot locates the misclassfied cell in purple and orange colors. As discussed above, the model failed to correctly classfy these due to the similarity of the chose features between the benigh and maglignant cells. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
